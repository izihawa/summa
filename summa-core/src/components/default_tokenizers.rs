use std::collections::HashSet;

use tantivy::tokenizer::{LowerCaser, RawTokenizer, RemoveLongFilter, SimpleTokenizer, StopWordFilter, TextAnalyzer, WhitespaceTokenizer};

use super::tokenizers::{DictTokenizer, HtmlTokenizer, Tokenizer};

/// List of stop words mixed for multiple languages
pub const STOP_WORDS: [&str; 321] = [
    "a",
    "an",
    "and",
    "are",
    "as",
    "at",
    "be",
    "by",
    "for",
    "from",
    "if",
    "in",
    "is",
    "it",
    "of",
    "on",
    "or",
    "s",
    "that",
    "the",
    "these",
    "this",
    "to",
    "was",
    "were",
    "which",
    "with",
    "aber",
    "alle",
    "allem",
    "allen",
    "aller",
    "alles",
    "als",
    "also",
    "am",
    "an",
    "ander",
    "andere",
    "anderem",
    "anderen",
    "anderer",
    "anderes",
    "anderm",
    "andern",
    "anderr",
    "anders",
    "auch",
    "auf",
    "aus",
    "bei",
    "bin",
    "bis",
    "bist",
    "da",
    "dann",
    "der",
    "den",
    "des",
    "dem",
    "das",
    "dass",
    "daß",
    "derselbe",
    "derselben",
    "denselben",
    "desselben",
    "demselben",
    "dieselbe",
    "dieselben",
    "dasselbe",
    "dazu",
    "dein",
    "deine",
    "deinem",
    "deinen",
    "deiner",
    "deines",
    "denn",
    "derer",
    "dessen",
    "dich",
    "dir",
    "du",
    "dies",
    "diese",
    "diesem",
    "diesen",
    "dieser",
    "dieses",
    "doch",
    "dort",
    "durch",
    "ein",
    "eine",
    "einem",
    "einen",
    "einer",
    "eines",
    "einig",
    "einige",
    "einigem",
    "einigen",
    "einiger",
    "einiges",
    "einmal",
    "er",
    "ihn",
    "ihm",
    "es",
    "etwas",
    "euer",
    "eure",
    "eurem",
    "euren",
    "eurer",
    "eures",
    "für",
    "gegen",
    "gewesen",
    "hab",
    "habe",
    "haben",
    "hat",
    "hatte",
    "hatten",
    "hier",
    "hin",
    "hinter",
    "ich",
    "mich",
    "mir",
    "ihr",
    "ihre",
    "ihrem",
    "ihren",
    "ihrer",
    "ihres",
    "euch",
    "im",
    "in",
    "indem",
    "ins",
    "ist",
    "jede",
    "jedem",
    "jeden",
    "jeder",
    "jedes",
    "jene",
    "jenem",
    "jenen",
    "jener",
    "jenes",
    "jetzt",
    "kann",
    "kein",
    "keine",
    "keinem",
    "keinen",
    "keiner",
    "keines",
    "können",
    "könnte",
    "machen",
    "man",
    "manche",
    "manchem",
    "manchen",
    "mancher",
    "manches",
    "mein",
    "meine",
    "meinem",
    "meinen",
    "meiner",
    "meines",
    "mit",
    "muss",
    "musste",
    "nach",
    "nicht",
    "nichts",
    "noch",
    "nun",
    "nur",
    "ob",
    "oder",
    "ohne",
    "sehr",
    "sein",
    "seine",
    "seinem",
    "seinen",
    "seiner",
    "seines",
    "selbst",
    "sich",
    "sie",
    "ihnen",
    "sind",
    "so",
    "solche",
    "solchem",
    "solchen",
    "solcher",
    "solches",
    "soll",
    "sollte",
    "sondern",
    "sonst",
    "um",
    "und",
    "uns",
    "unsere",
    "unserem",
    "unseren",
    "unser",
    "unseres",
    "unter",
    "viel",
    "vom",
    "von",
    "vor",
    "während",
    "waren",
    "warst",
    "weg",
    "weil",
    "weiter",
    "welche",
    "welchem",
    "welchen",
    "welcher",
    "welches",
    "wenn",
    "werde",
    "werden",
    "wie",
    "wieder",
    "wir",
    "wird",
    "wirst",
    "wo",
    "wollen",
    "wollte",
    "würde",
    "würden",
    "zu",
    "zum",
    "zur",
    "zwar",
    "zwischen",
    "и",
    "в",
    "во",
    "не",
    "что",
    "он",
    "на",
    "я",
    "с",
    "со",
    "как",
    "а",
    "то",
    "все",
    "она",
    "так",
    "его",
    "но",
    "да",
    "ты",
    "к",
    "у",
    "же",
    "вы",
    "за",
    "бы",
    "по",
    "ее",
    "мне",
    "было",
    "вот",
    "от",
    "о",
    "из",
    "ему",
    "ей",
    "им",
    "de",
    "la",
    "que",
    "el",
    "en",
    "y",
    "a",
    "los",
    "del",
    "se",
    "las",
    "por",
    "un",
    "para",
    "con",
    "una",
    "su",
    "al",
    "lo",
    "como",
    "más",
    "pero",
    "sus",
    "le",
    "ya",
    "o",
    "este",
    "sí",
    "lt",
    "gt",
    "amp",
];

/// Instantiate default tokenizers
pub fn default_tokenizers() -> [(String, TextAnalyzer); 7] {
    let summa_tokenizer = TextAnalyzer::builder(Tokenizer)
        .filter(RemoveLongFilter::limit(100))
        .filter(LowerCaser)
        .filter(StopWordFilter::remove(STOP_WORDS.map(String::from).to_vec()))
        .build();
    let summa_dict_tokenizer = TextAnalyzer::builder(DictTokenizer::new()).build();
    let summa_html_tokenizer = TextAnalyzer::builder(HtmlTokenizer::new(
        HashSet::from_iter(vec![
            "formula".to_string(),
            "figure".to_string(),
            "math".to_string(),
            "ref".to_string(),
            "table".to_string(),
        ]),
        HashSet::from_iter(vec![
            "sup".to_string(),
            "sub".to_string(),
            "i".to_string(),
            "b".to_string(),
            "u".to_string(),
            "scp".to_string(),
            "tt".to_string(),
        ]),
    ))
    .filter(RemoveLongFilter::limit(100))
    .filter(LowerCaser)
    .filter(StopWordFilter::remove(STOP_WORDS.map(String::from).to_vec()))
    .build();
    let summa_without_stop_words_tokenizer = TextAnalyzer::builder(Tokenizer).filter(RemoveLongFilter::limit(100)).filter(LowerCaser).build();
    let default_tokenizer = TextAnalyzer::builder(SimpleTokenizer::default())
        .filter(RemoveLongFilter::limit(100))
        .filter(LowerCaser)
        .filter(StopWordFilter::remove(STOP_WORDS.map(String::from).to_vec()))
        .build();
    let whitespace_tokenizer = TextAnalyzer::builder(WhitespaceTokenizer::default()).filter(LowerCaser).build();
    let raw_tokenizer = TextAnalyzer::builder(RawTokenizer::default()).filter(LowerCaser).build();
    [
        ("summa".to_owned(), summa_tokenizer),
        ("summa_dict".to_owned(), summa_dict_tokenizer),
        ("summa_html".to_owned(), summa_html_tokenizer),
        ("summa_without_stop_words".to_owned(), summa_without_stop_words_tokenizer),
        ("default".to_owned(), default_tokenizer),
        ("raw".to_owned(), raw_tokenizer),
        ("whitespace".to_owned(), whitespace_tokenizer),
    ]
}
